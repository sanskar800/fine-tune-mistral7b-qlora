# -*- coding: utf-8 -*-
"""FineTune Mistral-7B using QLoRA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16m2mGl1FprtYGGGGslcf10e853jRBcL-
"""

!pip install -q -U pip
!pip install -U "transformers>=4.46.0" "accelerate>=0.34.0" "peft>=0.13.0" "trl==0.26.0" "datasets>=3.0.0" "bitsandbytes>=0.43.0"

!pip install -q triton

import torch
import os
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model, PeftModel
from datasets import load_dataset
from trl import SFTTrainer, SFTConfig

print(f"CUDA Available: {torch.cuda.is_available()}")
print(f"CUDA Version: {torch.version.cuda}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")

MODEL_NAME = "mistralai/Mistral-7B-Instruct-v0.2"
DATASET_NAME = "databricks/databricks-dolly-15k"
OUTPUT_DIR = "./mistral-7b-dolly-qlora"
ADAPTER_DIR = "./mistral-7b-dolly-adapter"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,  # Nested quantization for memory efficiency
)

# LoRA hyperparameters (targeting Mistral architecture)
lora_config = LoraConfig(
    r=16,  # Low-rank dimension
    lora_alpha=32,  # Scaling factor
    target_modules=[
        "q_proj",  # Query projection
        "k_proj",  # Key projection
        "v_proj",  # Value projection
        "o_proj",  # Output projection
        "gate_proj",  # MLP gate
        "up_proj",  # MLP up
        "down_proj",  # MLP down
    ],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)

sft_config = SFTConfig(
    output_dir=OUTPUT_DIR,
    max_length=128,
    dataset_text_field="text",
    packing=False,
    fp16=False,             # Use float16 (Compatible with T4)
    bf16=True,            # Disable bfloat16 (Not supported on T4)
    num_train_epochs=1,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    optim="paged_adamw_32bit",
    report_to="none",
)

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
)

# Prepare model for k-bit training
model = prepare_model_for_kbit_training(model)

# Apply LoRA
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

dataset = load_dataset(DATASET_NAME, split="train")

dataset = dataset.shuffle(seed=42).select(range(1500))

len(dataset)

dataset[0]

def format_instruction(sample):
    """
    Formats Dolly dataset entries into instruction-response pairs.
    Format: [INST] instruction + context [/INST] response
    """
    instruction = sample["instruction"]
    context = sample.get("context", "")
    response = sample["response"]

    # Construct prompt (Mistral-Instruct format)
    if context:
        prompt = f"[INST] {instruction}\n\nContext: {context} [/INST] {response}"
    else:
        prompt = f"[INST] {instruction} [/INST] {response}"

    return {"text": prompt}

dataset = dataset.map(format_instruction, remove_columns=dataset.column_names)

print(dataset[0]['text'])

dtypes = set(p.dtype for p in model.parameters())
print(f"Model parameters dtypes: {dtypes}")

trainer = SFTTrainer(
    model=model,
    args=sft_config,
    train_dataset=dataset,
    processing_class=tokenizer,
)

trainer.train()

trainer.model.save_pretrained(ADAPTER_DIR)
tokenizer.save_pretrained(ADAPTER_DIR)

del trainer
del model
torch.cuda.empty_cache()

base_model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    quantization_config=bnb_config,
    device_map={"": 0},
    trust_remote_code=True,
)

model = PeftModel.from_pretrained(base_model, ADAPTER_DIR)
model.eval()

tokenizer = AutoTokenizer.from_pretrained(ADAPTER_DIR)

def generate_response(prompt, max_new_tokens=256):
    """
    Generate a response for a given instruction prompt.
    """
    # Format prompt in Mistral-Instruct style
    formatted_prompt = f"[INST] {prompt} [/INST]"

    # Tokenize
    inputs = tokenizer(formatted_prompt, return_tensors="pt").to("cuda")

    # Generate
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=0.7,
            top_p=0.9,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id,
        )

    # Decode and return only the generated response
    full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    response = full_text.split("[/INST]")[-1].strip()
    return response

test_prompts = [
    "Write a short poem about machine learning.",
    "Explain quantum computing in simple terms.",
    "What are the benefits of fine-tuning large language models?",
]

print("Testing fine-tuned model:\n")
for i, prompt in enumerate(test_prompts, 1):
    print(f"Example {i}:")
    print(f"Prompt: {prompt}")
    response = generate_response(prompt)
    print(f"Response: {response}")
    print("-" * 80 + "\n")